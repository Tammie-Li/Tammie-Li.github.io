<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results">
  <meta property="og:title" content="sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results"/>
  <meta property="og:description" content="sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results"/>
  <meta property="og:url" content="Tammie-Li.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1400"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results">
  <meta name="twitter:description" content="sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>sEMG-GF</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h3 class="xtitle is-1 publication-title">
            sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results</h3>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="#" target="_blank">Hongxin Li</a>
              </span>
                <span class="author-block">
                  <a href="https://github.com/Tammie-Li" target="_blank">Jingsheng Tang</a>
                </span>
                <span class="author-block">
                  <a href="https://github.com/Tammie-Li" target="_blank">Yaru Liu</a><sup>*</sup>
                </span>
                  <span class="author-block">
                    <a href="https://github.com/Tammie-Li" target="_blank">Xuechao Xu</a>
                  </span>
                  <span class="author-block">
                    <a href="hhttps://github.com/Tammie-Li" target="_blank">Wei Dai</a>
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/Tammie-Li" target="_blank">Junhao Xiao</a>
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/Tammie-Li" target="_blank">Huimin Lu</a>
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/Tammie-Li" target="_blank">Zongtan Zhou</a>
                  </span>
                  </div>
                  <br>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      National University of Defense Technology &nbsp;&nbsp;&nbsp;
                      <br>submitted to IEEE TII</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates corresponding author</small></span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2411.14131" target="_blank"
                        class="external-link ">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Tammie-Li/sEMG-GF" target="_blank"
                    class="external-link ">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              <span class="link-block">
                  <a href="https://pan.baidu.com/s/1yqxHlpqqJAmCEwqMDQ-_1w" target="_blank"
                  class="external-link ">
                  <span>ðŸ§¿ Data(password: 5435) </span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In sensitive scenarios, such as meetings, negotiations, and team sports, messages must be conveyed without detection by non-collaborators. 
            Previous methods, such as encrypting messages, eye contact, and micro-gestures,  had problems with either inaccurate information transmission 
            or leakage of interaction intentions. To this end, a novel gesture-free hand intention recognition scheme was proposed, that adopted surface 
            electromyography(sEMG) and isometric contraction theory to recognize different hand intentions without any gesture. Specifically, this work 
            includes four parts: (1) the experimental system, consisting of the upper computer software, self-conducted myoelectric watch, and sports 
            platform, is built to get sEMG signals and simulate multiple usage scenarios; (2) the paradigm is designed to standard prompt and collect 
            the gesture-free sEMG datasets. Eight-channel signals of ten subjects were recorded twice per subject at about 5-10 days intervals; (3) the 
            toolbox integrates preprocessing methods (data segmentation, filter, normalization, etc.), commonly used sEMG signal decoding methods, and 
            various plotting functions, to facilitate the research of the dataset; (4) the benchmark results of widely used methods are provided. 
            The results involve single-day, cross-day, and cross-subject experiments of 6-class and 12-class gesture-free hand intention when subjects 
            with different sports motions. To help future research, all data, hardware, software, and methods are open-sourced on the following website: click here. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper abstract -->
<section class="section hero is-light" style="background: white;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental System</h2>
          <div class="content has-text-justified">
            <img src="static/images/system.png"> </img>
            <p>
              Experimental system for sEMG-based gesture-free hand intention recognition. It comprises the self-conducted myoelectric wristband, the matched host
 computer software, and the Umay U3H sports platform. The myoelectric wristband is responsible for recording real-time sEMG signals; The host computer
 software has two pages, one is designed for data parameter setting and visualization, and another is for paradigm parameter settings and task prompts; The
 sports platform simulates different application scenarios.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light" style="background: white;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Paradigm</h2>
        <div class="content has-text-justified">
          <img src="static/images/paradigm.png"> </img>
          <p>
            The sEMG-based gesture-free hand intention recognition paradigm.
            Each experiment includes 12 blocks and each block includes 12 trials. The
            motion speed corresponds to each block and the hand force mode corresponds
            to each trial are different. The specific correspondence is shown in the table.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper abstract -->
<section class="section hero is-light" style="background: white;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset</h2>
        <div class="content has-text-justified">
          <img src="static/images/dataset.png"> </img>
          <p>
            Dataset format. The dataset includes data from two experiments
            involving ten subjects. Each experiment records a source file of shape (T,
            C), where T and C denote the number of sampling points and channels
            respectively. There are a total of 15 channels, and the meaning of each channel
            is shown in the table.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper abstract -->
<section class="section hero is-light" style="background: white;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Benchmark Results</h2>
        <div class="content has-text-justified">
          <img src="static/images/benchmark_results.png"> </img>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{
        title={sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results},
        author={Li, Hongxin and Tang, Jingsheng and Xu, Xuechao and Dai, Wei and Liu, Yaru and Xiao, Junhao and Lu, Huimin and Zhou, Zongtan},
        year={2024},
        eprint={2411.12194},
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/vinthony/project-page-template">modification version</a> of <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> from <a href="https://github.com/vinthony">vinthony</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
